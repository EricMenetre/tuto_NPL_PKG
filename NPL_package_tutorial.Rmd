---
title: "NPL package tutorial"
author: "Eric Ménétré"
date: "10/08/2020"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
---
```{r include=FALSE}
library(knitr)
```


The NPL package was created to help researchers from the NeuroPsychoLinguistic lab of the University of Geneva to perform behavioral and EEG analyses as well as helping to report the results and get the code easier to read. However, the package can be useful for any researcher in cognitive psychology.

A list of the included functions and their brief description is available on the GitHub page dedicated to the NPL package: https://github.com/EricMenetre/NPL.

# Package installation

Before installing or using the NPL package, please make sure that all the following packages are installed on your computer and are up to date.

```{code}
install.packages("ggplot2")
install.packages("dplyr")
install.packages("tidyr")
install.packages("tibble")
install.packages("ggpubr")
install.packages("bannerCommenter")
install.packages("lme4")
install.packages("lmerTest")
install.packages("car")
install.packages("emmeans")
install.packages("cAIC4")
install.packages("nlme")
install.packages("DescTools")
install.packages("stringr")
install.packages("RColorBrewer")
install.packages("devtools")

```

Then load the devtools package using the following command line:

```{code}
library("devtools")
```

And execute the following command:

```{code}
install_github("EricMenetre/NPL")
```

To use the NPL package, simply load it as any other package using:

```{r}
library(NPL)
```

The package will be improved over time, please update regularly your package version using the command line given at the third step. 
If you encounter difficulties to install these packages, please update your R and Rstudio versions. If the problem persists, be sure that the path where the package is saved does not contains accents. To find out where the packages downloaded from GitHub are saved use the `?install_github()` command (after loading the package devtools). If the path contains accents, for PC owners, create a temporary folder on the root directory of your C disk, and use the `.libPaths("C:/your_new_folder")` command and repeat the installation process. When the process is done, copy the NPL folder from the temporary folder to your R library. You will find the path in the `?install_github()` command. For mac user, the problem did not occured yet. If so, please contact Eric Ménétré.

If you notice a bug or you have trouble using a function, please contact me at: Eric.Menetre@unige.ch

Creating such a package necessitated a lot of time and efforts. If you used this package in your analyses, Please, cite it : 

```{r message=FALSE, warning=FALSE}
citation("NPL")
```


If you struggle with the use of some function, first refer to the help function. A detailed explanation on how to use the function and what should be the content of each argument is detailed there. To make the help appear, type ` help("nameofthefunction")`

Try for example to run:

```{code}
help("date.banner")
```

# Included functions and their usage

All the data used in the following tutorial is available on the [Github repo](https://github.com/EricMenetre/tuto_NPL_PKG). To download the repo, click on code(green button) and dowload Zip.

The description of the functions and their usage is didived in three sections. First the function designed to help performing behavioral analyses, then the function dedicated to EEG analyses and finaly the generic functions usable to help writing in Rmarkdown or reporting statistical results. For each function, detailed example and and explanations will be provided. 

## Behavioral analyses

To illustrate the following functions, a dataframe containing reaction time is provided in the [tutorial GitHub repo](https://github.com/EricMenetre/tuto_NPL_PKG). The file is named data_Stroop_RT.xlsx.

To import this file in R, use:

```{r}
library(readxl)
data_Stroop_RT <- read_excel("data_Stroop_RT.xlsx")
data_Stroop_RT$trigger <- as.character(data_Stroop_RT$trigger) # To fcilitate the analyses
data_Stroop_RT$group <- as.character(data_Stroop_RT$group)# To fcilitate the analyses
```

The dataframe contains several columns: 

```{r}
str(data_Stroop_RT)
```

- **subject**: contains the subjects names. All the observation of one subject contain the same value.
- **group**: in this study we were interested in understanding the age group differences of the Stroop effect on behavioral and EEG data. group represents the age group of the subject: either children (10_13); adolescents (16_18); young adults (20_30); middle age adults (40_50); first elderly group (60_70) and second elderly group (70_80)
- **item**: name of the item, for example jauneenbleu.bmp, i.e. yellow in blue
- **item.distr**: since in the Stroop task, the subject is asked to name in which color the words are presented, the distractor of the item is the color word.
- **item.color**: color in which the item is written.
- **trigger**: condition sent as trigger on the continuous EEG file. 1 = neutral; 2 = congruent; 3 = incongruent.
- **pres.order**: presentation order
- **RT**: RT extracter from the audio recording using the Checkvocal software (oral Stroop task)
- **CR**: correct responses. 1 = correct; 0 = incorrect
- **error.type**: type of error commited by the subject. W = wrong; wrcor = wrong but corrected himself, NR = no response, ...

Before starting, we need to remove the incorrect responses in the RT. Checkvocal puts negative values for items judged as incorrect and -1000 for no response trials. Moreover, we want to remove all the trials which have RT <= 200ms since it is very probably not reflecting the real cognitive mechanisms at work in the task. To remove these values, two methods can be used. Either using the base R package or using the [tidyverse](https://www.tidyverse.org/). The latter is a group of packages designed to facilitate data analyses. For base R user, switching to tidyverse might take a bit of time but I can guarantee that it is worth it. Hence, we will mainly use tidyverse syntax in this tutorial. This syntax uses a lot of pipe operators `%>%` which means "take the results of the previous operation on the left of the operator and put it as first argument (input) in the function on the right of the operator", the point being to articulate different functions without saving the intermediate steps. 

```{r, message=FALSE, warning=FALSE}
library(dplyr)
data_Stroop_RT <- data_Stroop_RT%>%
  mutate(RT = ifelse(RT > 200, RT, NA))

```



### clean.sd

RT are usually not normally distributed. In the present data frame, we can see that

```{r, warning=FALSE}
library(ggplot2)

med_RT <- median(data_Stroop_RT$RT, na.rm = T)
moy_RT <- mean(data_Stroop_RT$RT, na.rm = T)
ggplot(data_Stroop_RT, aes(x = RT))+ 
  geom_density(fill = "dodgerblue", alpha = 0.3, size = 1)+
  geom_vline(xintercept = med_RT, linetype = 2)+
  geom_vline(xintercept = moy_RT)+
  theme_minimal()+
  labs(title = "Distribution of the RT variable")

```

the reaction times are not equally distributed from each side of the median (dotted line). Two options are possible here. Either use non-parametric, generalized linear methods (using Gamma distribution), or cut the extreme values on the right side of the distribution. However, since there is a variability across groups or subjects, the cleaning of the extreme reaction times needs to be done per group or per subject. Again, two solutions are possible. Either use Excel, calculate one average and standard deviation per subject and manually change the RT to missing values for the values outranging the limits, or use the `clean.sd()` function directly in R, which is specifically designed to automatize this process. 

Here are the arguments of the function: 

```{r warning=FALSE}
args(clean.sd)
```

- df.var.val: Dataframe$column containing the RT
- df.var.group: Dataframe$column containing the groupping variable for example the age groups. If your experiment does not include groups, set this argument to "none". Default is "none"
- n.sd: Number of standard deviations above the mean where to cut
- data: Name of the dataframe containing the data
- fill: Value to replace the extreme values by. The default is NA

Let's apply this function to the data: 

```{r}
clean.sd(df.var.val = data_Stroop_RT$RT,
         df.var.group = data_Stroop_RT$group,
         n.sd = 1.5,
         data = data_Stroop_RT,
         fill = NA)
```

The function returns several outputs. First histograms and QQ-plots of the distribution of the RT before and after cleaning, and a table containing the superior and inferior limits were also been created. It also returns a message specifying that the work is done and the results have been checked for the first two groups. The same message specifies that a vector named clean.val has been created in the environement and we need to add this vector as a new variable to our data frame (and to keep things clean, we will remove this vector from the environement):

```{r}
data_Stroop_RT$RT_clean <- clean.val
rm(clean.val)

str(data_Stroop_RT)
```

Here the function was applied on the group level. It is also possible to apply it on the subject level, by specifying the subject variable as df.var.group.

```{r warning=FALSE}
med_RT <- median(data_Stroop_RT$RT_clean, na.rm = T)
moy_RT <- mean(data_Stroop_RT$RT_clean, na.rm = T)
ggplot(data_Stroop_RT, aes(x = RT_clean))+ 
  geom_density(fill = "dodgerblue", alpha = 0.3, size = 1)+
  geom_vline(xintercept = med_RT, linetype = 2)+
  geom_vline(xintercept = moy_RT)+
  theme_minimal()+
  labs(title = "Distribution of the RT variable")
```

Now the distribution is almost perfectly normally distributed, and the mean (solid line) and the median (dotted line) are closer to each other.



### clean.subj.group

Another question which might occur in data analysis is: "do I need to exclude some of the subjects which have too fast or too slow RT ?". To help answering this question, one could use the `clean.subj.group()` function. Here is an example for the group of young adults. Note that the data argument in the ggplot function has been removed compared  to the previous plots. Since we used the pipe operator, the data are the results of the previous command lines).  

```{r, warning=FALSE, message=FALSE}
data_Stroop_RT%>%
  filter(group == "20_30")%>%
ggplot(aes(x = subject, y = RT))+
  geom_boxplot()+
  theme_minimal()+
  labs(title = "Boxplot showing the differences in RT among subjects - young adults group")+
  theme(axis.text.x = element_text(angle = 45))

```

The arguments of this function are:

```{r}
args(clean.subj.group)
```

- subj.var: Subject variable. Use the raw name of the column e.g. subjects instead of data$subjects without "" either.
- RT.var: Variable containing the reaction times. Use the raw name of the column e.g. RT instead of data$RT without "" either.
- N.sd: Number of standard deviation above and under the mean where to put the rejection boundary.
- data: Name of the dataframe containing the experimental data.
- groupping: Specify TRUE if results need to be differentiated by group. Default is FALSE. If set to TRUE, specify the group.var argument.
- group.var: Groupping variable for example age group. Use the raw name of the column e.g. group instead of data$group without "" either. The default is NULL, use it like this without groupping variable in your data if groupping = FALSE. This argument is linked to the groupping one. if groupping = TRUE, specify the group.var argument.


```{r}
clean.subj.group(subj.var = subject,
                 RT.var = RT_clean,
                 N.sd = 2,
                 data = data_Stroop_RT,
                 groupping = TRUE,
                 group.var = group)
```

The function returns a data frame as output. The output can be saved in an object using the following method:

```{r}
which_to_remove <- clean.subj.group(subj.var = subject,
                                    RT.var = RT_clean,
                                    N.sd = 2,
                                    data = data_Stroop_RT,
                                    groupping = TRUE,
                                    group.var = group)
```

Now let's look at the output. it contains the mean of the group without this subject, with the inferior and superior limit of the group again without this subject, and then the mean of the subject. Finaly, a column called "criterion" tells if the subject can be kept or needs to be excluded.



### exp_plot_LMM

Now that the data are properly cleaned, it is ok to run some mixed models analyses on them. However, a last check might be useful before beginning. The `exp_plots_LMM()` returns some useful plots to perform before running linear mixed models analyses.
Be careful, this function is informative only for linear models.

```{r}
args(exp_plots_LMM)

```

- data: data frame containing the behavioral results
- DV: dependant variable under the form data$DV
- ID: ID variable containing the subjetcs names under the form data$ID
- class: variable containing the items or other second level variable under the form data$class

```{r, message=FALSE, warning=FALSE}
exp_plots_LMM(data = data_Stroop_RT,
              DV = data_Stroop_RT$RT_clean,
              ID = data_Stroop_RT$subject,
              class = data_Stroop_RT$item.color)
```

The function returns a plot figure containing four parts: 
- a histogram of the dependant variable
- a boxplot of the dependant variable
- one boxplot per subject to assess the variability between the subjects
- one boxplot per color included in the Stroop task to assess variability between colors (since the class variable was attributed to this factor. Generally this represents the second random effect after subjects, usually items).



### LMM_check

After performing a mixed model analysis, it is very important to check if the residus are normally distributed. To do so, either retrieve each plot by its own command line or use the `LMM_check()` function. As the previous one, this function is informative only for linear models.

Model selection method is documented in this [cheatsheet](https://github.com/EricMenetre/R-codes/blob/master/CheatSheet%20Linear%20Mixed%20Models.pdf).

We will assume that all the procedure has been followed and led us to the following model, including the main effects and interaction between trigger and age groups, and as random intercepts, the subjects and colors to be named: 
```{r, message=FALSE, warning=FALSE}
library(lme4)
library(lmerTest)

my_model <- lmer(RT_clean ~ trigger*group + (1|subject)+(1|item.color), data = data_Stroop_RT)
anova(my_model)
summary(my_model)

```

Here are the arguments of the function: 

```{r}
args(LMM_check)
```
- model: model built from the `lmer()` function.

```{r, warning=FALSE, message=FALSE}
LMM_check(my_model)
```

The function returns some plots. The number depends on the number of random effects. Here, item.color and subject were used as random effects. The plots from the first row should show normal distributions. It is difficult to see it in the item.color description which has only four dimensions. The two plots in the second row represents the distribution of the residuals, i.e. the difference between the actual values and the prediction, while the fourth plot examines the evolution of the residuals across the predicted values, or the heteroscedasticity. This plot should look like a cloud, but not a cone. 



### ICC_ranef
An often forgotten step in the analysis of mixed models is the interpretation of the intra-class correlation. This measure translates the cohesion of the data in one variable used as random effect in a mixed model. If the ICC is large, this means that there are large differences among the different level of the variable. A low ICC shows a good cohesion between the levels of the variable.

The arguments of the function are:
```{r}
args(ICC_ranef)
```

- model: model built from the `lmer()` function.

```{r, warning=FALSE, message=FALSE}
ICC_ranef(my_model)
```

The function returns a data frame containing the names of the random variables with their respective ICC. Here the subject variable explains 34.68% of the total variance of the DV. item.color shows a much better cohesion with an ICC of 0.69%. A high ICC motivates the importance to add a certain factor as random effect.



### mod_fitting

The hierarchical approach is often recognized as the gold standard regarding the selection of the mixed model to use. However, this approach is not very convenient, necessitating for the user to compare sometimes a dozen of models. To facilitate this process, the user could use the `mod_fitting()` function.

Let's imagine that five models were needed to reach the model my_model used in the `exp_plots_LMM()`, `LMM_check()` and `ICC_ranef()` functions.

```{r}

mod_0 <- lmer(RT_clean~ 1 + (1|subject)+(1|item.color), data = data_Stroop_RT, REML = FALSE)
mod_1 <- lmer(RT_clean~ trigger + (1|subject)+(1|item.color), data = data_Stroop_RT, REML = FALSE)
mod_2 <- lmer(RT_clean~ group + (1|subject)+(1|item.color), data = data_Stroop_RT, REML = FALSE)
mod_3 <- lmer(RT_clean~ trigger+group + (1|subject)+(1|item.color), data = data_Stroop_RT, REML = FALSE)
my_model <- lmer(RT_clean ~ trigger*group + (1|subject)+(1|item.color), data = data_Stroop_RT, REML = FALSE)
```

The selection of the best model among these is mainly based on the AIC and BIC information. To compare these models, we can use the `mod_fitting()` function:

```{r}
args(mod_fitting)
```

- models: a list (as `list()`) of lmer or glmer models


To use the function:

```{r, message=FALSE, warning=FALSE}

list_models <- list(m0 = mod_0, m1 = mod_1, m2 = mod_2, m3 = mod_3, m4 = my_model)
mod_fitting(list_models)
```

The function returns a dataframe arranging the models by the smallest AIC. Here the fourth model (my_model) seems to be the one with the best fitting.
The dataframe includes as well the information of the loglikelihood and df_resid. If the models is REML, the cAIC is estimated with its DF. To conclude, the model type and the REML information are mentioned as well as the formula. 
Be careful, this might take some time, if the REML parameters of the model are set to TRUE. Here parameters were set to FALSE to save computation time.

### cross_anova_models

Another option to compare models between them is to perform a chi-squared test between models fitting information. This can be achieved using the `anova()` function. However, this function can compare only models two by two. The `cross_anova_models()` function helps interpreting the results by designing a matrix confronting the anova p-values model by model, facilitating the interpretation. To use this function on the data here, we will use the list of models used in the `mod_fitting()` function.

This function take as argument:
```{r}
args(cross_anova_models)
```

- models: a list (as `list()`) of lmer or glmer models

To use the function: 

```{r}
cross_anova_models(list_models)
```

From the m0, when compared to the m1, m1 seems to have a better fitting. m1 is however not better than m2 but m3 seems better than m1. m4 is in the end better than m3. As for the previous function, the best model is the fourth one.


### std_error

Oddly, in the base package, no function is designed to calculate the standard error of an array. In the NPL package, the `std_error()` function calculates it as: 

```{code}
SE <- sd(data)/sqrt(nrow(data))
```

This function takes as arguments:

```{r}
args(std_error)
```

- x: a numerical array.
- na.rm: remove the NA ? default is FALSE.


For example, let's estimate the standard error of the accuracy column in our dataframe: 

```{r}
std_error(data_Stroop_RT$CR)
```

standard error can replace the standard deviation when the data are not normally distributed, for example, when reporting or plotting accuracy results.

### round_df

The `round()` function designed in the base R package allows the user to round an array. It can be applied to an entire dataframe, however, if one of the variables contains strings or logical, the function returns an error. The `round_df()` function uses the `round()` function but only on the columns which contain numerical values, avoiding the error.

This function takes as arguments:

```{r}
args(round_df)
```

- df: a dataframe
- digits: number of digits after the decimal point to round to


Let's try to apply the function to the dataframe
```{r}
round_df(data_Stroop_RT, 2)
```


## EEG analyses

### convert.eph.ep
Cartool, as well as many other softwares, can convert files from one format to another. All the function presented here require the data to be in the .ep format. usually, the .eph format is used in Cartool when dealing with epochs. The `convert.eph.ep()` function converts .eph files to the .ep format.

```{r}
args(convert.eph.ep)
```

- file: the path to the file to convert

```{r message=FALSE, warning=FALSE}
convert.eph.ep("convert.ep.eph/eph_to_convert_to_ep.eph")
```

Now, in the same folder as the original file, a new file with the extension "_converted" has been created

### convert.eph.ep.folder
The previous function allows to transform .eph files into .ep files. The following function extands it be applied to an entire folder. It is the equivalent of the batch option in Cartool.

```{r}
args(convert.eph.ep.folder)
```

- path: path to the folder containing all the .eph files. The best practice to avoid mistakes is to set the working directory to the path where the .eph files are stored and use the `getwd()` function to access it.

```{r message=FALSE, warning=FALSE, results="hide"}
# First set the working directory to the folder containing all the .eph files
setwd("eph_files")
# Run the function indicating the path to the working directory using the getwd() function.
convert.eph.ep.folder(getwd())
```


### create.tva.files
.tva files are text files read by Cartool during the averaging process. These files tell Cartool which trial are incorrect and need to be excluded, and specify the latencies for the response-aligned averaging in epochs. To create the .tva files, either open your behavioral results .xlsx file in Excel and copy paste for each subject the accuracy, trigger and RT information in a new Excel file and save it as .tva. This method is highly time consuming and prone to errors during the copy-paste process. The `create.tva.files()` allows the user to create automatically these .tva files from the behavioral results dataframe.

```{r}
args(create.tva.files)
```

- data: Name of the data frame containing the results
- subj.var: Name of the subject's ID variable (tidy format, i.e. variable name in the data frame without $ or "")
- CR.var: Name of the accuracy variable (tidy format)
- RT.var: Name of the RT variable (tidy format)
- trig.var: Name of the variable containing the triggers (tidy format)
- path.save: Path to the folder where to save the .tva files

Let's use the function on the Stroop data (dataframe presented for the Behavioral analyses). But first, we need to create the RT_TVA column containing the RT reduced by 100ms to avoid including the beginning of the articulation on the EEG epochs.

```{r message=FALSE, warning=FALSE}
# Creation of the RT_TVA colum
data_Stroop_RT$RT_TVA <- data_Stroop_RT$RT_clean-100
# Creation of the CR_TVA column and transformation of the variables to numerical values
data_Stroop_RT <- data_Stroop_RT%>%
  mutate(CR_TVA = ifelse(is.na(RT_TVA), 0,1),
         CR_TVA = as.numeric(CR_TVA),
         trigger = as.numeric(trigger))

# Application of the create.tva.files() function.
create.tva.files(data = data_Stroop_RT,
                 subj.var = subject,
                 CR.var = CR_TVA,
                 RT.var = RT_TVA,
                 trig.var = trigger,
                 path.save = "tva.files")

data_Stroop_RT$trigger <- as.character(data_Stroop_RT$trigger) # Retransform trigger as character
```

In less than a minute, 151 .tva files were created in the tva.files folder. Also beware of your variables format, which should all be numerical.

### delete.spaces.STEN
To import the data into the STEN software, the files should be under the .ep format, rereferenced to average and the names should not contain any space. To rename the files, one can either use a .bat script from Windows to extract the name of the files in the folder, copy that in Excel, rename and use another .bat file to rename all the files, or use the `delete.spaces.STEN()` function.

```{r}
args(delete.spaces.STEN)
```

- path: path to the folder to rename.

First let's extract the names of the files in the delete.spaces.STEN folder:

```{r}
list.files("delete.spaces.STEN/Original_files_names")

```

as we can see, the names contain some spaces. To remove them, we will use the `delete.spaces.STEN()` function.

```{r}
library(stringr)
delete.spaces.STEN("delete.spaces.STEN")
```

And now let's see the files names again: 

```{r}
list.files("delete.spaces.STEN")
```

All the spaces were replaced by an underscore. A similar function was written to prepare .eph files to the analyses in Ragu on Matlab. Ask me and I can provide it to you. 

### import.epochs.ep
To analyze data in R, the format is very important. To visualize and analyze directly EEGs in R, the format of the .ep files needs to be changed to make them readble by R. Also, generally, the researcher does not want to import a single epoch but rather all the epochs for a condition or an age group. The import.epochs.ep function allows to import epochs directly into R in the right format.

```{r}
args(import.epochs.ep)
```

- path: path to the folder containing the epochs under the .ep format
- N_electrodes: number of electrodes included in the .ep files. The number of electrodes should be the same for all the files.

```{r message=FALSE, echo=FALSE}
import.epochs.ep(path = "import.epochs.ep",
                 N_electrodes = 128)
```

After running the function, two elements were created. The first one is a list object named subj_list, containing all the files separately. The second one is a dataframe, named data_groupped, containing all the data in one big dataframe.


`r kable(head(data_groupped))`

The dataframe contains all the electrodes in columns. The last two columns contain the time frame information and each file is named "subj_1" to "subj_n".

Now let's imagine we want to plot the data for an electrode. Here X129 contains the time information, X1 is the first electrode, and V130 contains the subjects names
```{r}
ggplot(data_groupped, aes(x = X129, y = X1, color = V130))+
  geom_line()+
  geom_vline(xintercept = 25)+
  theme_minimal()+
  labs(x = "TF", y = "amplitude", color = "Subjects", title = "Trace of a posterior electrode across subjects")
  
```

In this plot, the traces of this electrode over time are shown per subject. The vertical lign represents the onset of the stimulus.


### subtract.eeg
In some cases it might be useful to subtract two EEGs. This can be achieved in a non-intuitive way in Cartool or with the `subtract.eeg()` function.

```{r}
args(subtract.eeg)
```

- file1: first file to subtract (imported and transformed by the `import.epochs.ep()` function)
- file2: second file to subtract (imported and transformed by the `import.epochs.ep()` function)
- path_save: path and file name where to save the result of the subtraction.

Be careful, the function always performs file2 - file1. Moreover, the coding is not very efficient, which can result in a long computation time and memory for large files.

Let's try this out on the data sotred in the subtract.eeg folder. First, to use this function, the data need to be imported using the `import.epochs.ep()` function. Then the information added by the `import.epochs.ep()` function need to be removes (i.e. the TF and subjects information). 

```{r message=FALSE, warning=FALSE}
# Importation using the import.epochs.ep() function
import.epochs.ep(path = "subtract.eeg",
                 N_electrodes = 128)

# transformation of the data by separating the elements contained in subj_list and taking only the EEG data from the columns 1 to 128.
trigger_2 <- subj_list[[1]][,1:128] 
trigger_3 <- subj_list[[2]][,1:128]

# Perform the subtraction of the two files
subtract.eeg(file1 = trigger_2,
             file2 = trigger_3,
             path_save = "subtracted_EEG/subtracted_EEG.ep")
```

The function returns an object called subtracted EEG in the global environement and writes a .ep file with the subtraction.

### tanova.ragu.to.cartool
When performing a Tanova in Ragu, the results can be exported as a .txt file. However, it might be handy to be able to open this file in Cartool to better visualize the data along with the STEN results for example. To do so, one can use the function `tanova.ragu.to.cartool()`

```{r}
args(tanova.ragu.to.cartool)
```

- file: path to the text file containing the results of the tanova from Ragu.
- sig_cut_off: Significance threshold according to which dichotomize the data. Default is 0.05

```{r warning=FALSE, message=FALSE}
tanova.ragu.to.cartool("tanova.ragu.to.cartool/tanova_results_2_3_FW.txt")
```

The function created in the working directory as many files as the number of comparisons present in the tanova .txt file. Here there was two main effects and their interaction, hence the function wrote three .ep files containing the results. It is also noteworthy to mention that the results are dichotomized to be more easily interpreted in Cartool. The default value is 0.05 but it can be set to 0.01, 0.001 or any other value if necessary. 

### find_max_amp_diff
When performing peak analysis, it is sometimes difficult to find the best electrode on which to perform the analysis. The `find_max_amp_diff()` function helps the user by showing the electrodes on which the difference between two conditions is maximal.

```{r}
args(find_max_amp_diff)
```

- data: a dataset imported with the `import.epochs.ep()` function 
- condition_var: variable of data containing the different conditions for each subject. Use the tidy format (simply the column name without $ or data[])
- time_var: Variable containing the time information, either in time frames or in ms. Use the tidy format (simply the column name without $ or data[])
- N_electrodes: Number of electrodes used in the file.

The function compares all electrodes according to the difference between their AUC (area under the curve). For now, the function takes only datasets with two conditions. If your dataset has a higher number of conditions, please consider applying this function two conditions by two conditions. Be careful, a minimal difference of AUC does not necessarily mean that there is no difference between two conditions. If the temporal window is large enough, two curves intersecting in an X shape might result to a minimal differences. Differences in time lag between two condition will also have a small AUC difference. This function helps you to chose the right electrodes but do not use it blindly !

```{r message=FALSE, warning=FALSE}

import.epochs.ep(path = "find_max_amp_diff",
                 N_electrodes = 128)

peak_analysis_diff <- find_max_amp_diff(data = data_groupped,
                  condition_var = V130,
                  time_var = X129,
                  N_electrodes = 128)


```

The function returns a dataframe containing all the electrodes names, the AUC for the first condition, the AUC of the second condition and their difference. The results are arranged according to the bigger AUC difference between two electrodes. Since the `import.epochs.ep()` function automatically names the EEG files "subj_1" to "subj_n" the conditions are named as so. However, this can be changed either using regular expressions or using the combination of the `mutate()` and `case_when()` functions from the dplyr package:

```{r warning=FALSE, message=FALSE}
# Using regular expressions
data_groupped$V130 <- str_replace(data_groupped$V130, "subj", "EEG") # Here the subj part of the name is replaced by EEG
head(data_groupped$V130)

# Using the mutate() and case_when() functions
import.epochs.ep(path = "find_max_amp_diff", 
                 N_electrodes = 128) # Re-importing the data to get the original names again.

data_groupped <- data_groupped%>%mutate(V130 = case_when(V130 == "subj_1" ~ "First EEG",
                                        V130 == "subj_2" ~ "Second EEG")) # Here all names have to be changed manually.

head(data_groupped$V130)

```


### import_stats_Ragu
When performing microstates analysis in Ragu, the software offers the possibility to perform statistics on the averages of each subject. In the Ragu interface, there is the possibility to obtain main effects and interaction effects but no post-hoc are available. That is why we usually perform statistics (RM anova or mixed models) by ourselve. Ragu allows to save a .xlsx file containing several sheets (one per map). Each line of the file represents a subjects and conditions are represented in columns. This configuration is not very convenient to analyze data in R. The `import_stats_Ragu()` function helps to put order in the data.

```{r}
args(import_stats_Ragu)
```

- path: path to the .xlsx file containing the microstates results
- n_maps: number of maps included in the segmentation
- n_conditions: number of triggers/conditions included in the analysis

Let's try to import the data from the import_stats_Ragu folder:
```{r}
import_stats_Ragu(path = "import_stats_Ragu/Stats_segmentation_I_C_N_FW_73tf_5maps.xlsx",
                  n_maps = 5,
                  n_conditions = 3)
```

The function returns a dataframe containing the data directly transformed to the tidy format.

#### import_stats_Cartool

As Ragu, Cartool does not returns output easily analyzable results documents when backfitting maps to the subjects averages. The `import_stats_Cartool()` helps at importing the results in a more R friendly format.

```{r}
args(import_stats_Cartool)
```
- path: a character vector containing the path to the directory where the .seg and the statistics are stored.

To use the function, simply indicate the path to the folder created when backfitting the segmentation to the subjects: 

```{r message=FALSE, warning=FALSE}
import_stats_Cartool(path = "import_stats_Cartool")
```

The function returns a dataframe containing all the information encompassed in all the .csv files from the Cartool output, combining the different groups.

### read_seg_file
For analysis or visualization purposes, one might want to import the data contained in the .seg file (containing the fitting information of each map on the GFP). These data can be imported in R using the `read_seg_file()` function.

```{r}
args(read_seg_file)
```
- path: path to the .seg file

Let's try this out:

```{r}
read_seg_file("read_seg_file__plot_microstates/Fit Maps FW_ICN_5maps.Group1.seg")

```

The function returns a dataframe containing all the information present in the seg file.

### plot_microstates
Plotting microstates is clearly not the best feature of neither Cartool or Ragu, and R is well known for its ability to do beautiful plots. However, plotting microstates results in R was not an easy task. The function `plot_microstates` helps performing the plots of the segmentation over the grand average, or through the individuals

```{r}
args(plot_microstates)
```

- path_.seg: path to the .seg file containing the information for the plot.
- vline_onset: this parameter allows to put a vertical line signaling the stimulus onset. Evidently, this argument is to use only with stimulus aligned data. Default is FALSE
- time_ms: this parameter allows the user to give the function a vector containing the timing information. If set to "none", the time frame information will be used. Default is "none"
- size_hist: to represent the different maps (colors) of the plot, an histogram is fitted under the GFP. Depending on the size of the output figure, some white spaces might be apparent if this size is too small. This parameter allows the user to adjust for that. The default is 2, which is a good compromise.
- palette: colors used in the plot use a palette system (group of colors systematically used for each map). When plotting stimulus aligned and response aligned data, one might prefer using another color set than the one implemented. Across all plots, maps number will have the same colors. by default the color palette is "default", another possibility is to set it on "alternative". This option will use different colors, which can be handy when dealing with a second .seg file, for example with response aligned. The color can also be customized by indicating the path to an Excel file containing new color names. The first column should contain the maps numbers and the second column the color names.

First let's define the time_ms vector. Here data were recorded at 512Hz and downsampled by 2, leading to a 256Hz sampling rate. This sampling rate indicates that one time frame represents approximatly 4ms. Moreover, the epochs contain a 100ms baseline, so approximatly 25TF. The time verctor here will hence be a sequence from -100 to 0 and another sequence from 0 to 520ms. This can be written as follow:

```{r}
time_in_ms <- c(seq(-100, 0, by = 4), seq(5,520, by = 4))
time_in_ms
```
To obtain the microstates plot, the following syntax can be used:

```{r warning=FALSE, message=FALSE}
list_plots <- plot_microstates(path_.seg = "read_seg_file__plot_microstates/Fit Maps FW_ICN_5maps.Group1.seg",
                 vline_onset = TRUE,
                 time_ms = time_in_ms,
                 size_hist = 2,
                 palette = "default")

```

The function returns a list of plots that can be either plotted individually: 

```{r}
list_plots[[1]]
```

or alltogether using the `do.call()` and `ggarrange()` function from the ggpubr package:

```{r fig.width= 10}
library(ggpubr)
do.call(ggarrange, list_plots)
```

To export the figure, use the `ggsave()` function. A tutorial on this function can be found [here](https://ggplot2.tidyverse.org/reference/ggsave.html).
Basically, in your R script, visualize the plot, and then use the ggsave function like this: 

```{code}
ggsave("my_nice_plot.jpg", dpi = 800) # The dpi argument is the resolution, the minimum for publication is usually 600
```


## Writing and reporting results


### number_to_word

Rmarkdown is a powerful tool to get some nice documents including the code and the results. More and more journals offer the authors to send their manuscript in .Rmd, which allows to be more directed towards open science. Rmarkdown allows to report results in the text directly. sometimes, depending on the results, the author don't want small numbers written in arabic numbers but rather in words. The `number_to_word()` function translates the results to be reported in words if smaller than 10 and if it is an integer.

Here is an example:

```{r}
my_number <- 4
number_to_word(my_number)
```

### report_results

As [pointed out recently](https://journals.sagepub.com/doi/full/10.1177/2515245918801915), a lot of errors on the scientific literature happen when rounding and reporting numbers from an analysis software to Word. The function `report_results()` helps the researcher to avoid these errors by pre-formating the results automatically. The researcher only has to copy-paste the output in Word. Since the police and its size are different between R and Word, copy it and paste it as values in Word, it will adapt to the style used in Word.

The function allows the user to report results from a Chi Squared, `anova()` of a model or `Anova()` of a model (from the car package), the `summary()` of a model or the post-hocs resutls from the `emmeans()` function.

```{r}
args(report_results)
```

- model_data: For "mainef_anova", "chisq", "mainef_Anova" and "summary" methods, put the statistical model. For the "emmeans" method, create an object as: your_model$pairwise..., and in a second step, convert this model in data frame using the as.data.frame() function and use this transformed model as argument.

- method: Statistical method employed: either "mainef_anova", in this case  `anova(model_data)` will be performed, "chisq", then the results of the model will be used, "mainef_Anova" the `Anova(model_data)` from the car package will be performed. The "summary" method uses the `summary(model_data)`. Some methods are implemented only for certain models. *mainef_anova* is implemented only for `lm()` and `lmer()` models. *chisq* only supports models from the `chisq.test()` function. *emmeans* supports only `emmeans()` models after transformation as described above. *mainef_Anova* supports only `glmer()` models. *summary* supports `lm()`, `lmer()`, and `glmer()` models.

Let's try out the function:

```{r, message=FALSE, warning=FALSE}
# On the Stroop dataset
library(car)
library(emmeans)
anova_model <- anova(my_model)
summary_model <- summary(my_model)
# Be careful, results could be misleading. With more than 3000 observation, the settings of the emmeans() function should be adapted. Since this adaptation increases the computational time, the setting will be set as default.
my_model <- lmer(RT_clean ~ trigger*group + (1|subject)+(1|item.color), data = data_Stroop_RT, REML = FALSE)
post_hocs <- emmeans(my_model, list(pairwise ~ group), adjust = "tukey")

# a glmer is needed for the Anova() function
my_glmer_model <- glmer(CR ~ trigger+group + (1|subject) + (1|item.color), family = "binomial", data = data_Stroop_RT)

# For the Chisq model
chi_var_1 <- c(40,12)
chi_var_2 <- c(156, 21)
data_chi <- data.frame(chi_var_1, chi_var_2)
chi_model <- chisq.test(data_chi)


```

#### anova() of the model
to use the `report_results()` function on the anova of the model, apply this syntax:

```{r}
report_results(model_data = my_model, method = "mainef_anova")
```

The outputs gives back a column with the effects names and a second column containing the output to report in Word. All numbers are rounded at two decimals except for the p-value which is at three. All values under 0.001 are noted "<0.001". 

#### summary() of a model

As for the `anova()` function, the `summary()` follows the same idea:

```{r}
report_results(model_data = my_model, method = "summary")
```


#### Anova() of the model

The same logic as above can be applied to the `Anova()` function. Be careful, the model needs to be a `glmer()` model. Otherwise, the following message appears: 

```{r}
report_results(model_data = my_model, method = "mainef_Anova")
```

 On a `glmer()` model, the results look like this: 
```{r}
report_results(model_data = my_glmer_model, method = "mainef_Anova")
```
 
#### emmeans() of a model

Using `report_results()` for `emmeans()` of a model is a bit more tricky. Since the output of the `emmeans()` function is under the form of a list, the user needs to pre-extract the data using the following steps: 

first create a new object and attribute it the object you previously created with the `emmeans()` function. Then add a $ sign and tap the tab key to make the suggestions menu appear, and select the option containing all the differences. Usually, it starts by pairwise... but not always. 

```{r}
post_hocs_transformed <- post_hocs$`pairwise differences of group`

```

Then the post_hocs_transformed object needs to ne converted as a dataframe:

```{r}
post_hocs_transformed <- as.data.frame(post_hocs_transformed)
```

And finaly use this new element in the `report_results()` function:

```{r}
report_results(model_data = post_hocs_transformed, method = "emmeans")
```

#### chisq.test() models

The `report_results()` function allows as well to report results from a Chi squared test: 

```{r}
report_results(model_data = chi_model, method = "chisq")
```

Be careful, in Word, the X letter needs to be replaced with a Chi.

### date.banner
The `date.banner()` function returns a banner in the console that the user can copy and paste in the code. The banner contains the date of the day.

The function does not take any argument.

```{r message=FALSE, warning=FALSE}
date.banner()
```

The point of this function is to help the user to organise its code based on the chronology.